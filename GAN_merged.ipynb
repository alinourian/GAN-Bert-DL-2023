{"cells":[{"cell_type":"markdown","metadata":{"id":"DxrLQBGQcFRd"},"source":["**To Do List**\n","\n","1. Implementation of the equation 5 for the variable `weight_classes_dataset`\n","2. Evaluation methods (e.g., Quadratic Weighted Kappa (QWK))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gAVYmG4P6ew4"},"outputs":[],"source":["# !pip install -U transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MuBm0aUeq6NO"},"outputs":[],"source":["# !pip install gdown"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tfEQUHkb3E17"},"outputs":[],"source":["import os\n","import re\n","import random\n","import time\n","import json\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import matplotlib.pyplot as plt\n","from transformers import AutoTokenizer, BertForSequenceClassification, GPT2Tokenizer, GPT2LMHeadModel\n","\n","# Set random seed for reproducibility\n","manualSeed = 0\n","print(\"Random Seed: \", manualSeed)\n","random.seed(manualSeed)\n","torch.manual_seed(manualSeed)"]},{"cell_type":"markdown","metadata":{"id":"FryLs-YP3E18"},"source":["## Inputs\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1qM_GAxH3E18"},"outputs":[],"source":["# Number of training epochs\n","num_epochs = 10\n","\n","# Learning rate for optimizers\n","lr = 0.0002\n","\n","# number of classes in the dataset\n","n_class_dataset = 6\n","\n","# name of each class in the dataset\n","label2class = {0: \"human\", 1: \"chatGPT\", 2: \"cohere\", 3: \"davinci\", 4: \"bloomz\", 5: \"dolly\"}\n","\n","# batch size\n","batch_size = 20\n","\n","# Output dimension of G1 network\n","d_out = 768\n","\n","# Dropout parameter\n","p_dropout = 0.5\n","\n","# beta1 and beta2 for the ADAM optimizers\n","betas_ADAM = (0.9, 0.999) # Note: no values reported in the paper\n","\n","# Decide which device we want to run on\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# lambda values in the loss functions\n","lambda_score = 1\n","lambda_feature_matching = 1"]},{"cell_type":"markdown","metadata":{"id":"iVh3ukO03E19"},"source":["## Data\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l96fHCDO50s0"},"outputs":[],"source":["if not os.path.exists(\"/content/SubtaskB\"):\n","    # https://github.com/mbzuai-nlp/SemEval2024-task8\n","    url = \"https://drive.google.com/drive/folders/11YeloR2eTXcTzdwI04Z-M2QVvIeQAU6-\"\n","    !gdown --folder $url"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wOcQTX3Y6aZ2"},"outputs":[],"source":["class SubtaskBDataset(Dataset):\n","    def __init__(self, file_path):\n","        self.data = []\n","        self.models = []\n","        self.sources = []\n","        with open(file_path, 'r') as file:\n","            for line in file:\n","                sample = json.loads(line)\n","                model = sample[\"model\"]\n","                source = sample[\"source\"]\n","                text = sample[\"text\"]\n","                y_label = sample[\"label\"]\n","                input_prompt = \"Model: {0}\".format(model)\n","                item = (text, y_label, input_prompt)\n","                self.data.append(item)\n","                self.models.append(model)\n","                self.sources.append(source)\n","        self.models = np.unique(self.models)\n","        self.sources = np.unique(self.sources)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","          return self.data[idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hJjZA9Sj-DpA"},"outputs":[],"source":["datasetTrain = SubtaskBDataset(\"/content/SubtaskB/subtaskB_train.jsonl\")\n","dataloaderTrain =  DataLoader(datasetTrain, batch_size=batch_size, shuffle=True)\n","\n","print(datasetTrain.__len__())\n","print(datasetTrain.models)\n","print(datasetTrain.sources)\n","print(datasetTrain.__getitem__(0))"]},{"cell_type":"code","source":["datasetVal = SubtaskBDataset(\"/content/SubtaskB/subtaskB_dev.jsonl\")\n","dataloaderVal =  DataLoader(datasetVal, batch_size=batch_size, shuffle=False)\n","\n","print(datasetVal.__len__())\n","print(datasetVal.models)\n","print(datasetVal.sources)\n","print(datasetVal.__getitem__(0))"],"metadata":{"id":"543A1pFQWBos"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nxKWYV2ZMRKR"},"source":["# The fine-tuned models/tokenizers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Upsj0Dl8-Thz"},"outputs":[],"source":["def loadGPT2(freeze=True, fine_tuned_model_path=\"alinourian/GPT2-SemEval2023\"):\n","    # Load the fine-tuned model and tokenizer\n","    modelGPT2 = GPT2LMHeadModel.from_pretrained(fine_tuned_model_path)\n","    tokenizerGPT2 = GPT2Tokenizer.from_pretrained(fine_tuned_model_path)\n","    if freeze:\n","        for param in modelGPT2.parameters():\n","            param.requires_grad = False\n","    return tokenizerGPT2, modelGPT2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9fD3kiMu0nB4"},"outputs":[],"source":["def loadBERT(freeze, fine_tuned_model_path=\"mohammadhossein/SemEvalTask8_SubTaskB\"):\n","    # Load the fine-tuned model and tokenizer\n","    modelBERT = BertForSequenceClassification.from_pretrained(fine_tuned_model_path)\n","    tokenizerBERT = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n","    if freeze:\n","        for param in modelBERT.parameters():\n","            param.requires_grad = False\n","    return tokenizerBERT, modelBERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4nxkALA_HmfK"},"outputs":[],"source":["def blockGPT2(input_prompt, tokenizerGPT2, modelGPT2):\n","    # Example of an input text : [\"Model: chatGPT\"]\n","    encoding = tokenizerGPT2(input_prompt, padding=True, return_tensors=\"pt\").to(device)\n","    # https://stackoverflow.com/questions/69609401/suppress-huggingface-logging-warning-setting-pad-token-id-to-eos-token-id\n","    output = modelGPT2.generate(\n","    **encoding,\n","    max_length=400,\n","    num_beams=1,\n","    temperature=0.8,\n","    do_sample=True,\n","    top_k=50,\n","    top_p=0.95,\n","    pad_token_id=tokenizerGPT2.eos_token_id\n","    )\n","    # A batch of generated texts\n","    generated_text = tokenizerGPT2.batch_decode(output, skip_special_tokens=True)\n","    # Remove the model names from the generated texts\n","    generated_text = [re.sub(r'Model:.+Text: ','', text, flags=re.IGNORECASE) for text in generated_text]\n","\n","    return generated_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"omROmKey1xlQ"},"outputs":[],"source":["def blockBERT(input_text, tokenizerBERT, modelBERT):\n","    # https://discuss.huggingface.co/t/how-to-get-cls-embeddings-from-bertfortokenclassification-model/9276/2\n","    inputs = tokenizerBERT(input_text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n","    outputs = modelBERT(**inputs, output_hidden_states=True)\n","    last_hidden_states = outputs.hidden_states[-1]\n","    CLS_hidden_states = last_hidden_states[:, 0, :] # (bs, 768)\n","    return CLS_hidden_states"]},{"cell_type":"markdown","metadata":{"id":"Tlffos1U3E2B"},"source":["## Generator\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QvLfXPlR3E2C"},"outputs":[],"source":["# # Generator G1, see figure 2a\n","# class GeneratorG1(nn.Module):\n","#     def __init__(self):\n","#         super(GeneratorG1, self).__init__()\n","#         self.main = nn.Sequential(\n","#             # input: Z\n","#             nn.Linear(d_in, d_out),\n","#             nn.LeakyReLU(0.2),\n","#             nn.Dropout(p=p_dropout),\n","#             nn.Linear(d_out, d_out)\n","#             # output: v_G\n","#         )\n","\n","#     def forward(self, input):\n","#         return self.main(input)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FWsi-Fel25Te"},"outputs":[],"source":["# Generator G3, see figure 2c\n","class GeneratorG3(nn.Module):\n","    def __init__(self):\n","        super(GeneratorG3, self).__init__()\n","        tokenizerGPT2, modelGPT2 = loadGPT2(freeze=True)\n","        tokenizerBERT, modelBERT = loadBERT(freeze=False)\n","        self.tokenizerGPT2 = tokenizerGPT2\n","        self.tokenizerBERT = tokenizerBERT\n","        self.modelGPT2 = modelGPT2\n","        self.modelBERT = modelBERT\n","\n","    def forward(self, input_prompt):\n","        generated_text = blockGPT2(input_prompt, self.tokenizerGPT2, self.modelGPT2)\n","        v_G = blockBERT(generated_text, self.tokenizerBERT, self.modelBERT)\n","        return generated_text, v_G"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z34RGb6x3E2H"},"outputs":[],"source":["# Create the generator\n","netG = GeneratorG3().to(device)\n","\n","# Print the model\n","print(netG)"]},{"cell_type":"markdown","metadata":{"id":"aJk9aHX93E2J"},"source":["## Discriminator\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rx2Kt_L03E2m"},"outputs":[],"source":["# Discriminator D, see figure 2d\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.seq1 = nn.Sequential(\n","            # input: v_G or v_B\n","            nn.Dropout(p=p_dropout),\n","            nn.Linear(d_out, d_out))\n","        self.seq2 = nn.Sequential(\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(p=p_dropout),\n","            nn.Linear(d_out, 1 + n_class_dataset), # +1 for the probability of this sample being fake/real.\n","            # output: logits, format: [fake score, dataset classes]\n","        )\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, input):\n","        features = self.seq1(input) # required for the feature matching loss\n","        logits = self.seq2(features)\n","        probs = self.softmax(logits)\n","        return features, logits, probs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jZvovxWO3E2o"},"outputs":[],"source":["# Create the Discriminator\n","netD = Discriminator().to(device)\n","\n","# Print the model\n","print(netD)"]},{"cell_type":"markdown","metadata":{"id":"f02WcyqL_J4L"},"source":["## BERT (red block)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JY1QYMMO_N0j"},"outputs":[],"source":["# BERT block for processing the real dataset, pictured in red in figure 1\n","tokenizerBERT_red, modelBERT_red = loadBERT(freeze=True)\n","modelBERT_red.to(device)"]},{"cell_type":"markdown","metadata":{"id":"-hBTnUBe3E2q"},"source":["## Training\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GBXVlgXj3E2p"},"outputs":[],"source":["criterionGAN = nn.BCEWithLogitsLoss()\n","criterionScore = nn.CrossEntropyLoss()\n","\n","# Setup Adam optimizers for both G and D\n","optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=betas_ADAM)\n","optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=betas_ADAM)\n","\n","# Establish convention for real and fake labels during training\n","real_label = 0.\n","fake_label = 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ofqjhRSp3E21"},"outputs":[],"source":["# Training Loop\n","\n","# Lists to keep track of progress\n","D_losses = []\n","G_losses = []\n","iters = 0\n","\n","time_start = time.time()\n","print(\"Starting Training Loop ...\")\n","# For each epoch\n","for epoch in range(num_epochs):\n","    # For each batch in the dataloader\n","    for i, data in enumerate(dataloaderTrain, 0):\n","\n","        # data is a list of [text, y_label, input_prompt]\n","        text = data[0] # text samples\n","        y_label = data[1].to(device) # true class labels\n","        input_prompt = data[2] # used for GPT2\n","        # output of the BERT module for real samples (CLS hidden state)\n","        v_B = blockBERT(text, tokenizerBERT_red, modelBERT_red)\n","\n","        ############################\n","        # (1) Update D network\n","        ############################\n","        ## Train with all-real batch\n","        netD.zero_grad()\n","\n","        labelGAN = torch.full((batch_size,), real_label, dtype=torch.float, device=device)\n","        _, logits_real, _ = netD(v_B)\n","        loss_D_real = criterionGAN(logits_real[:, 0], labelGAN)\n","        loss_D_score = criterionScore(logits_real[:, 1:], y_label)\n","        loss_D_real_and_score = loss_D_real + lambda_score*loss_D_score\n","        loss_D_real_and_score.backward() # Calculate gradients for D\n","\n","        ## Train with all-fake batch\n","        _, v_G = netG(input_prompt)\n","        labelGAN.fill_(fake_label)\n","        _, logits_fake, _ = netD(v_G.detach())\n","        loss_D_fake = criterionGAN(logits_fake[:, 0], labelGAN)\n","        loss_D_fake.backward() # Calculate gradients for D\n","\n","        loss_D_total = loss_D_real + loss_D_fake + lambda_score*loss_D_score\n","        optimizerD.step() # Update D\n","\n","        ############################\n","        # (2) Update G network\n","        ############################\n","        netG.zero_grad()\n","\n","        labelGAN.fill_(real_label)  # fake labels are real for generator cost\n","        # Since we just updated D, perform another forward pass\n","        features_real, _, _ = netD(v_B)\n","        features_fake, logits_fake, _ = netD(v_G)\n","        loss_G_caught = criterionGAN(logits_fake[:, 0], labelGAN)\n","        loss_G_feature_matching = torch.mean(torch.square(torch.mean(features_real, dim=0) - torch.mean(features_fake, dim=0)))\n","\n","        loss_G_total = loss_G_caught + lambda_feature_matching*loss_G_feature_matching\n","        loss_G_total.backward() # Calculate gradients for G\n","        optimizerG.step() # Update G\n","\n","        ############################\n","        # Training statistics\n","        ############################\n","        if i % 10 == 0:\n","            elapsed_time = time.time() - time_start\n","            print('[%3d/%3d][%3d/%3d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tElapsedTime: %.1f s'\n","                  % (epoch, num_epochs, i, len(dataloaderTrain), loss_D_total.item(), loss_G_total.item(), elapsed_time))\n","\n","        # Save Losses for plotting later\n","        D_losses.append(loss_D_total.item())\n","        G_losses.append(loss_G_total.item())\n","\n","        iters += 1\n","print(\"--------------------------------------------------------------------------------------------\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i_AwA9IR3ZTA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v4dUN0sV3pVN"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"AAk8ymdo3E22"},"source":["Training Loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UpB-CKwQ3E22"},"outputs":[],"source":["plt.figure(figsize=(6, 4))\n","plt.title(\"Generator and Discriminator Loss During Training\")\n","plt.plot(G_losses, label=\"G\")\n","plt.plot(D_losses, label=\"D\")\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fj7BWzz0Z7wK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aqvzxAvnlsZN"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Za9Ztv6Ylsbk"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/5f81194dd43910d586578638f83205a3/dcgan_faces_tutorial.ipynb","timestamp":1697365385522}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}