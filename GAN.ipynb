{"cells":[{"cell_type":"markdown","source":["**To Do List**\n","\n","1. Dataloader\n","2. Implementation of the equation 5 for the variable `weight_classes_dataset` defined in the **Inputs** section.\n","3. Implementation of the DistilGPT2 and the BERT modules\n","4. Evaluation methods (e.g., Quadratic Weighted Kappa (QWK))"],"metadata":{"id":"DxrLQBGQcFRd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tfEQUHkb3E17"},"outputs":[],"source":["import os\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Set random seed for reproducibility\n","manualSeed = 0\n","print(\"Random Seed: \", manualSeed)\n","random.seed(manualSeed)\n","torch.manual_seed(manualSeed)\n","torch.use_deterministic_algorithms(True) # Needed for reproducible results"]},{"cell_type":"markdown","metadata":{"id":"FryLs-YP3E18"},"source":["## Inputs\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1qM_GAxH3E18"},"outputs":[],"source":["# Number of training epochs\n","num_epochs = 10\n","\n","# Learning rate for optimizers\n","lr = 0.0002\n","\n","# number of classes in the dataset\n","n_class_dataset = 3\n","\n","# for the weighted cross entropy loss\n","weight_classes_dataset = torch.tensor(np.ones(n_class_dataset, dtype=np.float32))\n","\n","# Input dimension of G1 network, i.e., size of the input vector z\n","d_in = 100\n","\n","# Output dimension of G1 network\n","d_out = 768\n","\n","# Dropout parameter\n","p_dropout = 0.5\n","\n","# beta1 and beta2 for the ADAM optimizers\n","betas_ADAM = (0.9, 0.999) # Note: no values reported in the paper\n","\n","# Decide which device we want to run on\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# lambda values in the loss functions\n","lambda_score = 1\n","lambda_feature_matching = 1"]},{"cell_type":"markdown","metadata":{"id":"iVh3ukO03E19"},"source":["## Data\n","\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"MuBm0aUeq6NO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tlffos1U3E2B"},"source":["## Generator\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QvLfXPlR3E2C"},"outputs":[],"source":["# Generator G1, see figure 2a\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        self.main = nn.Sequential(\n","            # input: Z\n","            nn.Linear(d_in, d_out),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(p=p_dropout),\n","            nn.Linear(d_out, d_out)\n","            # output: v_G\n","        )\n","\n","    def forward(self, input):\n","        return self.main(input)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z34RGb6x3E2H"},"outputs":[],"source":["# Create the generator\n","netG = Generator().to(device)\n","\n","# netG.apply(weights_init)\n","\n","# Print the model\n","print(netG)"]},{"cell_type":"markdown","metadata":{"id":"aJk9aHX93E2J"},"source":["## Discriminator\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rx2Kt_L03E2m"},"outputs":[],"source":["# Discriminator D, see figure 2d\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.seq1 = nn.Sequential(\n","            # input: v_G or v_B\n","            nn.Dropout(p=p_dropout),\n","            nn.Linear(d_out, d_out))\n","        self.seq2 = nn.Sequential(\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(p=p_dropout),\n","            nn.Linear(d_out, 1 + n_class_dataset), # +1 for the probability of this sample being fake/real.\n","            # output: logits, format: [fake score, dataset classes]\n","        )\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, input):\n","        features = self.seq1(input) # required for the feature matching loss\n","        logits = self.seq2(features)\n","        probs = self.softmax(logits)\n","        return features, logits, probs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jZvovxWO3E2o"},"outputs":[],"source":["# Create the Discriminator\n","netD = Discriminator().to(device)\n","\n","# netD.apply(weights_init)\n","\n","# Print the model\n","print(netD)"]},{"cell_type":"markdown","metadata":{"id":"-hBTnUBe3E2q"},"source":["## Training\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GBXVlgXj3E2p"},"outputs":[],"source":["criterionGAN = nn.BCEWithLogitsLoss()\n","criterionScore = nn.CrossEntropyLoss(weight=weight_classes_dataset)\n","\n","# Setup Adam optimizers for both G and D\n","optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=betas_ADAM)\n","optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=betas_ADAM)\n","\n","# Establish convention for real and fake labels during training\n","real_label = 0.\n","fake_label = 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ofqjhRSp3E21"},"outputs":[],"source":["# Training Loop\n","\n","# Lists to keep track of progress\n","D_losses = []\n","G_losses = []\n","iters = 0\n","\n","print(\"Starting Training Loop ...\")\n","# For each epoch\n","for epoch in range(num_epochs):\n","    # For each batch in the dataloader\n","    for i, data in enumerate(dataloader, 0):\n","\n","        ############################\n","        # (1) Update D network\n","        ############################\n","        ## Train with all-real batch\n","        netD.zero_grad()\n","        # data is a list of [v_B, y_labels]\n","        v_B = data[0].to(device) # output of the BERT module for real essays (CLS hidden state)\n","        y_labels = data[1].to(device) # true class labels\n","        b_size = v_B.size(0)\n","        labelGAN = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n","        _, logits, _ = netD(v_B)\n","        loss_D_real = criterionGAN(logits[:, 0], labelGAN)\n","        loss_D_score = criterionScore(logits[:, 1:], y_labels)\n","\n","        ## Train with all-fake batch\n","        # Generate a batch of latent vectors\n","        input_noise = torch.randn(b_size, di, 1, 1, device=device)\n","        v_G = netG(input_noise)\n","        labelGAN.fill_(fake_label)\n","        _, logits, _ = netD(v_G.detach())\n","        loss_D_fake = criterionGAN(logits[:, 0], labelGAN)\n","        loss_D_real_and_fake = loss_D_real + loss_D_fake\n","\n","        loss_D_total = loss_D_real_and_fake + lambda_score*loss_D_score\n","        loss_D_total.backward() # Calculate gradients for D\n","        optimizerD.step() # Update D\n","\n","        ############################\n","        # (2) Update G network\n","        ############################\n","        netG.zero_grad()\n","        labelGAN.fill_(real_label)  # fake labels are real for generator cost\n","        # Since we just updated D, perform another forward pass\n","        features_real, _, _ = netD(v_B)\n","        features_fake, logits, _ = netD(v_G)\n","        loss_G_caught = criterionGAN(logits[:, 0], labelGAN)\n","        loss_G_feature_matching = torch.mean(torch.square(torch.mean(features_real, dim=0) - torch.mean(features_fake, dim=0)))\n","\n","        loss_G_total = loss_G_caught + lambda_feature_matching*loss_G_feature_matching\n","        loss_G_total.backward() # Calculate gradients for G\n","        optimizerG.step() # Update G\n","\n","        ############################\n","        # Training statistics\n","        ############################\n","        if i % 10 == 0:\n","            print('[%3d/%3d][%3d/%3d]\\tLoss_D: %.4f\\tLoss_G: %.4f'\n","                  % (epoch, num_epochs, i, len(dataloader), loss_D_total.item(), loss_G_total.item()))\n","\n","        # Save Losses for plotting later\n","        D_losses.append(loss_D_total.item())\n","        G_losses.append(loss_G_total.item())\n","\n","        iters += 1\n","print(\"--------------------------------------------------------------------------------------------\")"]},{"cell_type":"markdown","metadata":{"id":"AAk8ymdo3E22"},"source":["Training Loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UpB-CKwQ3E22"},"outputs":[],"source":["plt.figure(figsize=(6, 4))\n","plt.title(\"Generator and Discriminator Loss During Training\")\n","plt.plot(G_losses, label=\"G\")\n","plt.plot(D_losses, label=\"D\")\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","source":[],"metadata":{"id":"Fj7BWzz0Z7wK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aqvzxAvnlsZN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Za9Ztv6Ylsbk"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/5f81194dd43910d586578638f83205a3/dcgan_faces_tutorial.ipynb","timestamp":1697365385522}]}},"nbformat":4,"nbformat_minor":0}