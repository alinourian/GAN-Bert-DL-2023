{"cells":[{"cell_type":"markdown","source":["GAN-BERT (in Pytorch and compatible with HuggingFace) : https://github.com/crux82/ganbert-pytorch"],"metadata":{"id":"uitzgeVITk3j"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"gAVYmG4P6ew4"},"outputs":[],"source":["# !pip install -U transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tfEQUHkb3E17"},"outputs":[],"source":["import os\n","import re\n","import urllib.request\n","import random\n","import time\n","import json\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import matplotlib.pyplot as plt\n","from transformers import AutoTokenizer, BertForSequenceClassification, GPT2LMHeadModel, set_seed\n","\n","# Set random seed for reproducibility\n","manualSeed = 0\n","print(\"Random Seed: \", manualSeed)\n","random.seed(manualSeed)\n","np.random.seed(manualSeed)\n","torch.manual_seed(manualSeed)\n","set_seed(manualSeed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(manualSeed)"]},{"cell_type":"markdown","metadata":{"id":"FryLs-YP3E18"},"source":["## Inputs\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1qM_GAxH3E18"},"outputs":[],"source":["# Number of training epochs\n","num_epochs = 10\n","\n","# Learning rate for optimizers\n","lr = 5e-5\n","\n","# number of classes in the dataset\n","n_class_dataset = 6\n","\n","# name of each class in the dataset\n","label2class = {0: \"human\", 1: \"chatGPT\", 2: \"cohere\", 3: \"davinci\", 4: \"bloomz\", 5: \"dolly\"}\n","\n","# batch size\n","batch_size = 64\n","\n","# max sequence length for BERT models\n","max_length_bert = 64 # Similar to GAN-BERT\n","\n","# generator model\n","generator_type = 'G1'\n","assert generator_type in ['G1', 'G2', 'G3']\n","\n","# Input dimension of the G1 network (the latent vector z)\n","d_in = 100\n","\n","# Output dimension of the G1 network\n","d_out = 768\n","\n","# Dropout parameter\n","p_dropout = 0.2\n","\n","# beta1 and beta2 for the ADAM optimizers\n","betas_ADAM = (0.9, 0.999) # Note: no values reported in the paper\n","\n","# Number of GPUs available\n","ngpu = 1 # use two GPU for kaggle\n","\n","# Decide which device we want to run on\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# lambda values in the loss functions\n","lambda_score = 1\n","lambda_feature_matching = 1"]},{"cell_type":"code","source":["if generator_type in ['G2', 'G3']:\n","    raise NotImplementedError # For G3: we have to fine-tune distilGP2 model over a few labeled samples"],"metadata":{"id":"_PKg4nSAyePE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["length_dataset_train = 71027\n","length_dataset_val = 3000\n","labeled_fraction_train = 0.01 # select this parameter in range of [0, 1]\n","\n","# Split the training dataset into labeled and unlabeled samples\n","random_state_perm = np.random.RandomState(manualSeed)\n","n_labeled_samples = int(length_dataset_train*labeled_fraction_train)\n","print('labeled fraction: {:.2f} %'.format(labeled_fraction_train*100))\n","labeled_indices_train = random_state_perm.permutation(length_dataset_train)[:n_labeled_samples]\n","labeled_indices_val = np.arange(length_dataset_val)\n","\n","# Replicate labeled data to balance poorly represented datasets,\n","# e.g., less than 1% of labeled material\n","apply_balance_train = True\n","if apply_balance_train:\n","    balance_factor_train = int(1/labeled_fraction_train)\n","    balance_factor_train = 3*int(np.log2(balance_factor_train)) # with an additional factor of 3\n","    if balance_factor_train < 1:\n","        balance_factor_train = 1\n","    print('balance factor train: {0}'.format(balance_factor_train))\n","else:\n","    balance_factor_train = None"],"metadata":{"id":"kScXykuIKbBx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iVh3ukO03E19"},"source":["## Data\n","\n","\n"]},{"cell_type":"markdown","source":["https://github.com/mbzuai-nlp/SemEval2024-task8"],"metadata":{"id":"crnMHHpVX66q"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"l96fHCDO50s0"},"outputs":[],"source":["url_train = \"https://dl.dropboxusercontent.com/scl/fi/ux5ke4t5id230976pm0xt/subtaskB_train.jsonl?rlkey=l9tgwwhw75sbap08rap67r8vh&dl=0\"\n","urllib.request.urlretrieve(url_train, \"subtaskB_train.jsonl\")"]},{"cell_type":"code","source":["url_dev = \"https://dl.dropboxusercontent.com/scl/fi/sb4kzf4ifxzgn4ottz3am/subtaskB_dev.jsonl?rlkey=qqxkcjqh09ecjdji192h95opt&dl=0\"\n","urllib.request.urlretrieve(url_dev, \"subtaskB_dev.jsonl\")"],"metadata":{"id":"0tVUWTd9X4Xs"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wOcQTX3Y6aZ2"},"outputs":[],"source":["class SubtaskBDataset(Dataset):\n","    def __init__(self, file_path, labeled_indices, apply_balance=False, balance_factor=None):\n","        self.data = []\n","        self.models = []\n","        self.sources = []\n","        with open(file_path, 'r') as file:\n","            for i, line in enumerate(file, 0):\n","                sample = json.loads(line)\n","                model = sample[\"model\"]\n","                source = sample[\"source\"]\n","                text = sample[\"text\"]\n","                y_label = sample[\"label\"]\n","                input_prompt = \"Model: {0}\".format(model)\n","                is_labeled = (i in labeled_indices)\n","                item = (text, y_label, input_prompt, is_labeled)\n","                if apply_balance:\n","                    if is_labeled:\n","                        for _ in range(balance_factor):\n","                            self.data.append(item)\n","                    else:\n","                        self.data.append(item)\n","                else:\n","                    self.data.append(item)\n","                self.models.append(model)\n","                self.sources.append(source)\n","        self.models = np.unique(self.models)\n","        self.sources = np.unique(self.sources)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","          return self.data[idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hJjZA9Sj-DpA"},"outputs":[],"source":["datasetTrain = SubtaskBDataset(\"/content/subtaskB_train.jsonl\", labeled_indices=labeled_indices_train, apply_balance=apply_balance_train, balance_factor=balance_factor_train)\n","dataloaderTrain =  DataLoader(datasetTrain, batch_size=batch_size, shuffle=True)\n","\n","print(datasetTrain.__len__())\n","print(datasetTrain.models)\n","print(datasetTrain.sources)\n","print(datasetTrain.__getitem__(0))"]},{"cell_type":"code","source":["datasetVal = SubtaskBDataset(\"/content/subtaskB_dev.jsonl\", labeled_indices=labeled_indices_val, apply_balance=False, balance_factor=None)\n","dataloaderVal =  DataLoader(datasetVal, batch_size=batch_size, shuffle=False)\n","\n","print(datasetVal.__len__())\n","print(datasetVal.models)\n","print(datasetVal.sources)\n","print(datasetVal.__getitem__(0))"],"metadata":{"id":"543A1pFQWBos"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nxKWYV2ZMRKR"},"source":["# The fine-tuned models/tokenizers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Upsj0Dl8-Thz"},"outputs":[],"source":["def loadGPT2(model_path=\"alinourian/DistilGPT2-SemEval2024\"):\n","    # Load the fine-tuned model and tokenizer\n","    modelGPT2 = GPT2LMHeadModel.from_pretrained(model_path)\n","    tokenizerGPT2 = AutoTokenizer.from_pretrained(model_path)\n","    return tokenizerGPT2, modelGPT2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9fD3kiMu0nB4"},"outputs":[],"source":["def loadBERT(model_path=\"bert-base-cased\"):\n","    # We do not use the fine-tuned BERT model of \"mohammadhossein/SemEvalTask8_SubTaskB\"\n","    # Load the model and tokenizer\n","    modelBERT = BertForSequenceClassification.from_pretrained(model_path)\n","    tokenizerBERT = AutoTokenizer.from_pretrained(model_path)\n","    return tokenizerBERT, modelBERT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4nxkALA_HmfK"},"outputs":[],"source":["def blockGPT2(input_prompt, tokenizerGPT2, modelGPT2):\n","    # Example of an input text : [\"Model: chatGPT\"]\n","    encoding = tokenizerGPT2(input_prompt, padding=True, return_tensors=\"pt\").to(device)\n","    # https://stackoverflow.com/questions/69609401/suppress-huggingface-logging-warning-setting-pad-token-id-to-eos-token-id\n","    output = modelGPT2.generate(\n","    **encoding,\n","    max_length=400,\n","    num_beams=1,\n","    temperature=0.8,\n","    do_sample=True,\n","    top_k=50,\n","    top_p=0.95,\n","    pad_token_id=tokenizerGPT2.eos_token_id\n","    )\n","    # A batch of generated texts\n","    generated_text = tokenizerGPT2.batch_decode(output, skip_special_tokens=True)\n","    # Remove the model names from the generated texts\n","    generated_text = [re.sub(r'Model:.+Text: ','', text, flags=re.IGNORECASE) for text in generated_text]\n","\n","    return generated_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"omROmKey1xlQ"},"outputs":[],"source":["def blockBERT(input_text, tokenizerBERT, modelBERT):\n","    # https://discuss.huggingface.co/t/how-to-get-cls-embeddings-from-bertfortokenclassification-model/9276/2\n","    inputs = tokenizerBERT(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length_bert).to(device)\n","    outputs = modelBERT(**inputs, output_hidden_states=True)\n","    last_hidden_states = outputs.hidden_states[-1]\n","    CLS_hidden_states = last_hidden_states[:, 0, :] # (bs, 768)\n","    return CLS_hidden_states"]},{"cell_type":"markdown","metadata":{"id":"Tlffos1U3E2B"},"source":["## Generator\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QvLfXPlR3E2C"},"outputs":[],"source":["class GeneratorG1(nn.Module):\n","    def __init__(self):\n","        super(GeneratorG1, self).__init__()\n","        self.main = nn.Sequential(\n","            # input: Z\n","            nn.Linear(d_in, d_out),\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(p=p_dropout),\n","            nn.Linear(d_out, d_out)\n","            # output: v_G\n","        )\n","\n","    def forward(self, latent_vector):\n","        v_G = self.main(latent_vector)\n","        return v_G"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z34RGb6x3E2H"},"outputs":[],"source":["# Create the generator\n","if generator_type == 'G1':\n","    # Generator G1, see figure 2a\n","    netG = GeneratorG1()\n","    # Handle multi-GPU if desired\n","    if ngpu > 1:\n","        netG = nn.DataParallel(netG, device_ids=list(range(ngpu))).to(device)\n","    else:\n","        netG = netG.to(device)\n","    # Print the model\n","    print(netG)\n","elif generator_type == 'G3':\n","    # Generator G3, see figure 2c\n","    tokenizerGPT2, modelGPT2 = loadGPT2()\n","    # freeze the parameters of GPT2\n","    for param in modelGPT2.parameters():\n","        param.requires_grad = False\n","    tokenizerBERT_netG, modelBERT_netG = loadBERT()\n","    # Handle multi-GPU if desired\n","    if ngpu > 1:\n","        modelGPT2 = nn.DataParallel(modelGPT2, device_ids=list(range(ngpu))).to(device)\n","        modelBERT_netG = nn.DataParallel(modelBERT_netG, device_ids=list(range(ngpu))).to(device)\n","    else:\n","        modelGPT2 = modelGPT2.to(device)\n","        modelBERT_netG = modelBERT_netG.to(device)\n","    # Print the models\n","    print(modelGPT2)\n","    print(modelBERT_netG)\n","else:\n","    raise NotImplementedError"]},{"cell_type":"markdown","metadata":{"id":"aJk9aHX93E2J"},"source":["## Discriminator\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rx2Kt_L03E2m"},"outputs":[],"source":["# Discriminator D, see figure 2d\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.seq1 = nn.Sequential(\n","            # input: v_G or v_B\n","            nn.Dropout(p=p_dropout),\n","            nn.Linear(d_out, d_out))\n","        self.seq2 = nn.Sequential(\n","            nn.LeakyReLU(0.2),\n","            nn.Dropout(p=p_dropout),\n","            nn.Linear(d_out, 1 + n_class_dataset), # +1 for the probability of this sample being fake/real.\n","            # output: logits, format: [fake score, dataset classes]\n","        )\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, input):\n","        features = self.seq1(input) # required for the feature matching loss\n","        logits = self.seq2(features)\n","        probs = self.softmax(logits)\n","        return features, logits, probs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jZvovxWO3E2o"},"outputs":[],"source":["# Create the Discriminator\n","netD = Discriminator()\n","\n","# Handle multi-GPU if desired\n","if ngpu > 1:\n","    netD = nn.DataParallel(netD, device_ids=list(range(ngpu))).to(device)\n","else:\n","    netD = netD.to(device)\n","\n","# Print the model\n","print(netD)"]},{"cell_type":"markdown","metadata":{"id":"f02WcyqL_J4L"},"source":["## BERT (red block)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JY1QYMMO_N0j"},"outputs":[],"source":["# BERT block for processing the real dataset, pictured in red in figure 1\n","tokenizerBERT_red, modelBERT_red = loadBERT()\n","\n","# We don't freeze the parameters of modelBERT_red anymore\n","\n","# Handle multi-GPU if desired\n","if ngpu > 1:\n","    modelBERT_red = nn.DataParallel(modelBERT_red, device_ids=list(range(ngpu))).to(device)\n","else:\n","    modelBERT_red = modelBERT_red.to(device)"]},{"cell_type":"markdown","source":["## Accuracy"],"metadata":{"id":"aB9Hs-jlODCm"}},{"cell_type":"code","source":["def get_accuracy(dataloader):\n","    modelBERT_red.eval()\n","    netD.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for i, data in enumerate(dataloader, 0):\n","            text = data[0] # text samples\n","            y_label = data[1].to(device) # true class labels\n","            # output of the BERT module for real samples (CLS hidden state)\n","            v_B = blockBERT(text, tokenizerBERT_red, modelBERT_red)\n","            _, logits, _ = netD(v_B)\n","            _, predicted = torch.max(logits[:, 1:], 1)\n","            total += y_label.size(0)\n","            correct += (predicted == y_label).sum().item()\n","    accuracy = 100.0*correct/total\n","    modelBERT_red.train()\n","    netD.train()\n","    return accuracy"],"metadata":{"id":"v4sFEs0wOFMK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-hBTnUBe3E2q"},"source":["## Training\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GBXVlgXj3E2p"},"outputs":[],"source":["criterionGAN = nn.BCEWithLogitsLoss()\n","criterionScore = nn.CrossEntropyLoss()\n","\n","# Setup Adam optimizers for both G and D\n","D_vars = list(netD.parameters()) + list(modelBERT_red.parameters())\n","\n","if generator_type == 'G1':\n","    G_vars = list(netG.parameters())\n","elif generator_type == 'G3':\n","    G_vars = list(modelBERT_netG.parameters())\n","else:\n","    raise NotImplementedError\n","\n","optimizerD = optim.Adam(D_vars, lr=lr, betas=betas_ADAM)\n","optimizerG = optim.Adam(G_vars, lr=lr, betas=betas_ADAM)\n","\n","# Establish convention for real and fake labels during training\n","real_label = 0.\n","fake_label = 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ofqjhRSp3E21"},"outputs":[],"source":["# Training Loop\n","\n","iters = 0\n","time_start = time.time()\n","print(\"Starting Loop ...\")\n","# For each epoch\n","for epoch in range(num_epochs):\n","    # Validation Accuracy\n","    print('[%3d/%3d]\\tValidation Accuracy: %.2f' %(epoch, num_epochs, get_accuracy(dataloaderVal)))\n","\n","    # Training: For each batch in the dataloader\n","    for i, data in enumerate(dataloaderTrain, 0):\n","\n","        # data format: (text, y_label, input_prompt, is_labeled)\n","        text = data[0] # text samples\n","        y_label = data[1].to(device) # true class labels\n","        input_prompt = data[2] # used for GPT2\n","        is_labeled = data[3].to(device) # identifying labeled or unlabeled training data\n","        b_size = y_label.size(0)\n","\n","        # output of the BERT module for real samples (CLS hidden state)\n","        v_B = blockBERT(text, tokenizerBERT_red, modelBERT_red)\n","\n","        if generator_type == 'G1':\n","            # Generate batch of latent vectors\n","            latent_vector = torch.randn(b_size, d_in, device=device)\n","            v_G = netG(latent_vector)\n","        elif generator_type == 'G3':\n","            generated_text = blockGPT2(input_prompt, tokenizerGPT2, modelGPT2)\n","            v_G = blockBERT(generated_text, tokenizerBERT_netG, modelBERT_netG)\n","        else:\n","            raise NotImplementedError\n","\n","        features_real, logits_real, _ = netD(v_B)\n","        features_fake, logits_fake, _ = netD(v_G)\n","\n","        ############################\n","        # Loss evaluation\n","        ############################\n","        labelGAN_real = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n","        labelGAN_fake = torch.full((b_size,), fake_label, dtype=torch.float, device=device)\n","\n","        # It may be the case that a batch does not contain labeled examples,\n","        # so the \"supervised loss\" in this case is not evaluated\n","        if torch.sum(is_labeled) == 0:\n","            loss_D_score = 0\n","        else:\n","            loss_D_score = criterionScore(logits_real[:, 1:][is_labeled], y_label[is_labeled])\n","\n","        loss_D_real = criterionGAN(logits_real[:, 0], labelGAN_real)\n","        loss_D_fake = criterionGAN(logits_fake[:, 0], labelGAN_fake)\n","\n","        loss_D_total = loss_D_real + loss_D_fake + lambda_score*loss_D_score\n","\n","        loss_G_caught = criterionGAN(logits_fake[:, 0], labelGAN_real) # fake labels are real for generator cost\n","        loss_G_feature_matching = torch.mean(torch.square(torch.mean(features_real, dim=0) - torch.mean(features_fake, dim=0)))\n","\n","        loss_G_total = loss_G_caught + lambda_feature_matching*loss_G_feature_matching\n","\n","        ############################\n","        # Optimization\n","        ############################\n","        # Avoid gradient accumulation\n","        optimizerG.zero_grad()\n","        optimizerD.zero_grad()\n","\n","        # retain_graph = True is required since the underlying graph will be deleted after backward\n","        loss_G_total.backward(retain_graph=True)\n","        loss_D_total.backward()\n","\n","        optimizerG.step()\n","        optimizerD.step()\n","\n","        ############################\n","        # Training statistics\n","        ############################\n","        if i % 10 == 0:\n","            elapsed_time = time.time() - time_start\n","            print('[%3d/%3d][%3d/%3d]\\tLoss_D_total: %.4f\\tLoss_G_total: %.4f\\tloss_D_score: %.4f\\tElapsedTime: %.1f s'\n","                  % (epoch, num_epochs, i, len(dataloaderTrain), loss_D_total, loss_G_total, loss_D_score, elapsed_time))\n","\n","        iters += 1\n","print(\"--------------------------------------------------------------------------------------------\")"]},{"cell_type":"markdown","source":["Accuracy"],"metadata":{"id":"VXY_9sSLreUf"}},{"cell_type":"code","source":["get_accuracy(dataloaderVal)"],"metadata":{"id":"peeelXy2VCeO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v4dUN0sV3pVN"},"outputs":[],"source":["# get_accuracy(dataloaderTrain)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Za9Ztv6Ylsbk"},"outputs":[],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"jAPnU-3tiWVM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"w3U3aJI5iWXW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Gn-yotumiWZ_"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/5f81194dd43910d586578638f83205a3/dcgan_faces_tutorial.ipynb","timestamp":1697365385522}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}